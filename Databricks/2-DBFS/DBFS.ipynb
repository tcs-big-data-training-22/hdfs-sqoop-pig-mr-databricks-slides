{"cells":[{"cell_type":"markdown","source":["# Databricks File System (DBFS)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0795e2c-fee8-4621-9a9b-6a8615c24201"}}},{"cell_type":"markdown","source":["- Databricks File System (DBFS) is a distributed file system mounted into an Azure Databricks workspace and available on Azure Databricks clusters\n- DBFS is an abstraction on top of scalable object storage and offers the following benefits:\n    - Allows you to mount storage objects so that you can seamlessly access data without requiring credentials.\n    - Allows you to interact with object storage using directory and file semantics instead of storage URLs.\n    -Persists files to object storage, so you wonâ€™t lose data after you terminate a cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8545bd61-8878-49af-8938-343ec3339baf"}}},{"cell_type":"markdown","source":["## Important information about DBFS permissions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c134c00b-e576-4e8c-a6a3-edefa38c1243"}}},{"cell_type":"markdown","source":["- All users have read and write access to the objects in object storage mounted to DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836929e0-7517-4f68-b148-9b4344feb7cb"}}},{"cell_type":"markdown","source":["## DBFS root"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eef58933-0c06-4e22-aebb-58d574c5fea0"}}},{"cell_type":"markdown","source":["- The default storage location in DBFS is known as the DBFS root. Several types of data are stored in the following DBFS root locations:\n\n    - /FileStore: Imported data files, generated plots, and uploaded libraries.\n    - /databricks-datasets: Sample public datasets. See Special DBFS root locations.\n    - /databricks-results: Files generated by downloading the full results of a query.\n    - /databricks/init: Global and cluster-named (deprecated) init scripts.\n    - /user/hive/warehouse: Data and metadata for non-external Hive tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b7e7b03-2210-4d01-8670-6c9b8924ceb7"}}},{"cell_type":"markdown","source":["- In a new workspace, the DBFS root has the following default folders:\n\n<img src=\"https://docs.microsoft.com/en-us/azure/databricks/_static/images/getting-started/dbfs-root.png\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51364c5a-7da7-4822-89ee-844cb41e9931"}}},{"cell_type":"markdown","source":["## Mount object storage to DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fbe9ff0-1fd2-4365-bd6f-6bb598c711fe"}}},{"cell_type":"markdown","source":["- Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa524074-57aa-40b7-b306-363a2daf0e1e"}}},{"cell_type":"markdown","source":["## Access DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"095957d3-0888-4f8e-a97d-7cc9f9deff57"}}},{"cell_type":"markdown","source":["- The path to the default blob storage (root) is dbfs:/."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b8c208e-a90d-4c47-9951-98f0394f5f97"}}},{"cell_type":"markdown","source":["- The default location for %fs and dbutils.fs is root. Thus, to read from or write to root or an external bucket:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62927c29-30f3-4c93-87f9-d1132de0d687"}}},{"cell_type":"markdown","source":["## %fs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2aa2bad8-0430-47d6-a6cd-ae25f4ec687e"}}},{"cell_type":"code","source":["%fs ls /tmp/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"121aa3bd-a969-47ad-adfa-e125e3bdc7c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/hive/","hive/",0,1651997578000],["dbfs:/tmp/my_cloud_dir/","my_cloud_dir/",0,1652005374000],["dbfs:/tmp/my_new_file","my_new_file",32,1652005055000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/hive/</td><td>hive/</td><td>0</td><td>1651997578000</td></tr><tr><td>dbfs:/tmp/my_cloud_dir/</td><td>my_cloud_dir/</td><td>0</td><td>1652005374000</td></tr><tr><td>dbfs:/tmp/my_new_file</td><td>my_new_file</td><td>32</td><td>1652005055000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs mkdirs /tmp/my_cloud_dir"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb6fcac-05d4-45b9-b3c3-ea33b4850e6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/","databricks-datasets/",0,0],["dbfs:/databricks-results/","databricks-results/",0,0],["dbfs:/mnt/","mnt/",0,1651999893000],["dbfs:/tmp/","tmp/",0,1652005054000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/</td><td>mnt/</td><td>0</td><td>1651999893000</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td><td>1652005054000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.put(\"/tmp/test_dbfs.txt\", \"This is a file in cloud storage.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b138f1f6-151e-49e7-b729-bc999da3e030"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Wrote 32 bytes.\nOut[28]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 32 bytes.\nOut[28]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /tmp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e82fcd9-1171-41dd-b9b1-a319288a404c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/hive/","hive/",0,1651997578000],["dbfs:/tmp/my_cloud_dir/","my_cloud_dir/",0,1652005374000],["dbfs:/tmp/my_new_file","my_new_file",32,1652005055000],["dbfs:/tmp/test_dbfs.txt","test_dbfs.txt",32,1652005676000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/hive/</td><td>hive/</td><td>0</td><td>1651997578000</td></tr><tr><td>dbfs:/tmp/my_cloud_dir/</td><td>my_cloud_dir/</td><td>0</td><td>1652005374000</td></tr><tr><td>dbfs:/tmp/my_new_file</td><td>my_new_file</td><td>32</td><td>1652005055000</td></tr><tr><td>dbfs:/tmp/test_dbfs.txt</td><td>test_dbfs.txt</td><td>32</td><td>1652005676000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs cp /tmp/test_dbfs.txt /tmp/test_dbfs_cp.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6d31683-d99e-425f-ae20-9a2d2b278375"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res24: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res24: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Default location for dbutils.fs is root\ndbutils.fs.ls (\"/tmp/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcac8f95-e30e-4da1-aa6b-5e69f1e1c6e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: [FileInfo(path=&#39;dbfs:/tmp/hive/&#39;, name=&#39;hive/&#39;, size=0, modificationTime=1651997578000),\n FileInfo(path=&#39;dbfs:/tmp/my_cloud_dir/&#39;, name=&#39;my_cloud_dir/&#39;, size=0, modificationTime=1652005374000),\n FileInfo(path=&#39;dbfs:/tmp/my_new_file&#39;, name=&#39;my_new_file&#39;, size=32, modificationTime=1652005055000),\n FileInfo(path=&#39;dbfs:/tmp/test_dbfs.txt&#39;, name=&#39;test_dbfs.txt&#39;, size=0, modificationTime=1652005699000),\n FileInfo(path=&#39;dbfs:/tmp/test_dbfs_cp.txt&#39;, name=&#39;test_dbfs_cp.txt&#39;, size=0, modificationTime=1652005713000)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [FileInfo(path=&#39;dbfs:/tmp/hive/&#39;, name=&#39;hive/&#39;, size=0, modificationTime=1651997578000),\n FileInfo(path=&#39;dbfs:/tmp/my_cloud_dir/&#39;, name=&#39;my_cloud_dir/&#39;, size=0, modificationTime=1652005374000),\n FileInfo(path=&#39;dbfs:/tmp/my_new_file&#39;, name=&#39;my_new_file&#39;, size=32, modificationTime=1652005055000),\n FileInfo(path=&#39;dbfs:/tmp/test_dbfs.txt&#39;, name=&#39;test_dbfs.txt&#39;, size=0, modificationTime=1652005699000),\n FileInfo(path=&#39;dbfs:/tmp/test_dbfs_cp.txt&#39;, name=&#39;test_dbfs_cp.txt&#39;, size=0, modificationTime=1652005713000)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.put(\"/tmp/my_new_file\", \"This is a file in cloud storage.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"869891a9-ec66-4dfe-9c81-8c71c711d08a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3740128408545038&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>put<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/tmp/my_new_file&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;This is a file in cloud storage.&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.put.\n: org.apache.hadoop.fs.FileAlreadyExistsException: File already exists:/8190330192940305/tmp/my_new_file\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1842)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1677)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$3(DatabricksFileSystemV2.scala:646)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$2(DatabricksFileSystemV2.scala:643)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$withUserContextRecorded$2(DatabricksFileSystemV2.scala:1044)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:1017)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$1(DatabricksFileSystemV2.scala:642)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.create(DatabricksFileSystemV2.scala:642)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:142)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$put$1(DBUtilsCore.scala:260)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:91)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.put(DBUtilsCore.scala:257)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"org.apache.hadoop.fs.FileAlreadyExistsException: File already exists:/8190330192940305/tmp/my_new_file","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3740128408545038&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>put<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/tmp/my_new_file&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;This is a file in cloud storage.&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.put.\n: org.apache.hadoop.fs.FileAlreadyExistsException: File already exists:/8190330192940305/tmp/my_new_file\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1842)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1677)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$3(DatabricksFileSystemV2.scala:646)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$2(DatabricksFileSystemV2.scala:643)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$withUserContextRecorded$2(DatabricksFileSystemV2.scala:1044)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:1017)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$1(DatabricksFileSystemV2.scala:642)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.create(DatabricksFileSystemV2.scala:642)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:142)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$put$1(DBUtilsCore.scala:260)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:91)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.put(DBUtilsCore.scala:257)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Access files on the local filesystem"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73f370a6-0a24-4ebd-a13c-7d96784792db"}}},{"cell_type":"markdown","source":["- %fs and dbutils.fs read by default from root (dbfs:/). To read from the local filesystem, you must use file:/."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e32c5e48-f1e7-4f8e-9549-e3a7aee099f8"}}},{"cell_type":"code","source":["%fs ls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1954c394-a126-4465-a562-ef06360f73c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/","databricks-datasets/",0,0],["dbfs:/databricks-results/","databricks-results/",0,0],["dbfs:/mnt/","mnt/",0,1651999893000],["dbfs:/tmp/","tmp/",0,1652005713000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/</td><td>mnt/</td><td>0</td><td>1651999893000</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td><td>1652005713000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# With %fs and dbutils.fs, you must use file:/ to read from local filesystem"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12668bab-0156-4afa-bb02-00446ac36117"}}},{"cell_type":"code","source":["%fs ls file:/tmp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d360447d-0ca4-4ffc-b816-cb89c5150129"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-resolved.service-2YvTDg/","systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-resolved.service-2YvTDg/",4096,1652000084493],["file:/tmp/chauffeur-env.sh","chauffeur-env.sh",156,1652000090253],["file:/tmp/my_local_dir/","my_local_dir/",4096,1652005289956],["file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-ntp.service-YpvIfj/","systemd-private-5113780ac5c74ed29638f768d2efd6b2-ntp.service-YpvIfj/",4096,1652000084533],["file:/tmp/.XIM-unix/",".XIM-unix/",4096,1652000084485],["file:/tmp/tmp.PlGFmXpFnO","tmp.PlGFmXpFnO",0,1652000092309],["file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-apache2.service-XaQoci/","systemd-private-5113780ac5c74ed29638f768d2efd6b2-apache2.service-XaQoci/",4096,1652000093897],["file:/tmp/chauffeur-daemon.pid","chauffeur-daemon.pid",4,1652000092365],["file:/tmp/.X11-unix/",".X11-unix/",4096,1652000084485],["file:/tmp/.Test-unix/",".Test-unix/",4096,1652000084485],["file:/tmp/.font-unix/",".font-unix/",4096,1652000084485],["file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-logind.service-e9iyDg/","systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-logind.service-e9iyDg/",4096,1652000084557],["file:/tmp/hsperfdata_root/","hsperfdata_root/",4096,1652000100693],["file:/tmp/driver-env.sh","driver-env.sh",3280,1652000099137],["file:/tmp/my_new_file","my_new_file",40,1652005318345],["file:/tmp/Rserv/","Rserv/",4096,1652000144863],["file:/tmp/test_dbfs.txt","test_dbfs.txt",6,1652005644740],["file:/tmp/master-params","master-params",18,1652000092557],["file:/tmp/custom-spark.conf","custom-spark.conf",377,1652000090317],["file:/tmp/RtmpFwBmCF/","RtmpFwBmCF/",4096,1652000143367],["file:/tmp/chauffeur-daemon-params","chauffeur-daemon-params",22,1652000092149],["file:/tmp/spark-root-org.apache.spark.deploy.master.Master-1.pid","spark-root-org.apache.spark.deploy.master.Master-1.pid",4,1652000092773],["file:/tmp/.ICE-unix/",".ICE-unix/",4096,1652000084485],["file:/tmp/driver-daemon-params","driver-daemon-params",19,1652000100497],["file:/tmp/driver-daemon.pid","driver-daemon.pid",4,1652000100669]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-resolved.service-2YvTDg/</td><td>systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-resolved.service-2YvTDg/</td><td>4096</td><td>1652000084493</td></tr><tr><td>file:/tmp/chauffeur-env.sh</td><td>chauffeur-env.sh</td><td>156</td><td>1652000090253</td></tr><tr><td>file:/tmp/my_local_dir/</td><td>my_local_dir/</td><td>4096</td><td>1652005289956</td></tr><tr><td>file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-ntp.service-YpvIfj/</td><td>systemd-private-5113780ac5c74ed29638f768d2efd6b2-ntp.service-YpvIfj/</td><td>4096</td><td>1652000084533</td></tr><tr><td>file:/tmp/.XIM-unix/</td><td>.XIM-unix/</td><td>4096</td><td>1652000084485</td></tr><tr><td>file:/tmp/tmp.PlGFmXpFnO</td><td>tmp.PlGFmXpFnO</td><td>0</td><td>1652000092309</td></tr><tr><td>file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-apache2.service-XaQoci/</td><td>systemd-private-5113780ac5c74ed29638f768d2efd6b2-apache2.service-XaQoci/</td><td>4096</td><td>1652000093897</td></tr><tr><td>file:/tmp/chauffeur-daemon.pid</td><td>chauffeur-daemon.pid</td><td>4</td><td>1652000092365</td></tr><tr><td>file:/tmp/.X11-unix/</td><td>.X11-unix/</td><td>4096</td><td>1652000084485</td></tr><tr><td>file:/tmp/.Test-unix/</td><td>.Test-unix/</td><td>4096</td><td>1652000084485</td></tr><tr><td>file:/tmp/.font-unix/</td><td>.font-unix/</td><td>4096</td><td>1652000084485</td></tr><tr><td>file:/tmp/systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-logind.service-e9iyDg/</td><td>systemd-private-5113780ac5c74ed29638f768d2efd6b2-systemd-logind.service-e9iyDg/</td><td>4096</td><td>1652000084557</td></tr><tr><td>file:/tmp/hsperfdata_root/</td><td>hsperfdata_root/</td><td>4096</td><td>1652000100693</td></tr><tr><td>file:/tmp/driver-env.sh</td><td>driver-env.sh</td><td>3280</td><td>1652000099137</td></tr><tr><td>file:/tmp/my_new_file</td><td>my_new_file</td><td>40</td><td>1652005318345</td></tr><tr><td>file:/tmp/Rserv/</td><td>Rserv/</td><td>4096</td><td>1652000144863</td></tr><tr><td>file:/tmp/test_dbfs.txt</td><td>test_dbfs.txt</td><td>6</td><td>1652005644740</td></tr><tr><td>file:/tmp/master-params</td><td>master-params</td><td>18</td><td>1652000092557</td></tr><tr><td>file:/tmp/custom-spark.conf</td><td>custom-spark.conf</td><td>377</td><td>1652000090317</td></tr><tr><td>file:/tmp/RtmpFwBmCF/</td><td>RtmpFwBmCF/</td><td>4096</td><td>1652000143367</td></tr><tr><td>file:/tmp/chauffeur-daemon-params</td><td>chauffeur-daemon-params</td><td>22</td><td>1652000092149</td></tr><tr><td>file:/tmp/spark-root-org.apache.spark.deploy.master.Master-1.pid</td><td>spark-root-org.apache.spark.deploy.master.Master-1.pid</td><td>4</td><td>1652000092773</td></tr><tr><td>file:/tmp/.ICE-unix/</td><td>.ICE-unix/</td><td>4096</td><td>1652000084485</td></tr><tr><td>file:/tmp/driver-daemon-params</td><td>driver-daemon-params</td><td>19</td><td>1652000100497</td></tr><tr><td>file:/tmp/driver-daemon.pid</td><td>driver-daemon.pid</td><td>4</td><td>1652000100669</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs mkdirs file:/tmp/my_local_dir"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81f487cc-c720-4f6e-85c9-049f2fe7c606"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res4: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res4: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls (\"file:/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4498095-dfd0-4880-9e4c-561c85e86ba6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[17]: [FileInfo(path=&#39;file:/dev/&#39;, name=&#39;dev/&#39;, size=520, modificationTime=1652000076348),\n FileInfo(path=&#39;file:/mnt/&#39;, name=&#39;mnt/&#39;, size=4096, modificationTime=1652000085733),\n FileInfo(path=&#39;file:/databricks/&#39;, name=&#39;databricks/&#39;, size=4096, modificationTime=1652000092325),\n FileInfo(path=&#39;file:/tmp/&#39;, name=&#39;tmp/&#39;, size=4096, modificationTime=1652005289956),\n FileInfo(path=&#39;file:/root/&#39;, name=&#39;root/&#39;, size=4096, modificationTime=1652000288566),\n FileInfo(path=&#39;file:/var/&#39;, name=&#39;var/&#39;, size=4096, modificationTime=1652000069892),\n FileInfo(path=&#39;file:/lib64/&#39;, name=&#39;lib64/&#39;, size=4096, modificationTime=1652000051751),\n FileInfo(path=&#39;file:/BUILD&#39;, name=&#39;BUILD&#39;, size=190, modificationTime=1652000042907),\n FileInfo(path=&#39;file:/run/&#39;, name=&#39;run/&#39;, size=500, modificationTime=1652000401355),\n FileInfo(path=&#39;file:/boot/&#39;, name=&#39;boot/&#39;, size=4096, modificationTime=1652000069616),\n FileInfo(path=&#39;file:/sbin/&#39;, name=&#39;sbin/&#39;, size=12288, modificationTime=1652000069808),\n FileInfo(path=&#39;file:/lib/&#39;, name=&#39;lib/&#39;, size=4096, modificationTime=1652000069652),\n FileInfo(path=&#39;file:/opt/&#39;, name=&#39;opt/&#39;, size=4096, modificationTime=1652000065192),\n FileInfo(path=&#39;file:/etc/&#39;, name=&#39;etc/&#39;, size=4096, modificationTime=1652000084405),\n FileInfo(path=&#39;file:/srv/&#39;, name=&#39;srv/&#39;, size=4096, modificationTime=1652000069620),\n FileInfo(path=&#39;file:/proc/&#39;, name=&#39;proc/&#39;, size=0, modificationTime=1652000072280),\n FileInfo(path=&#39;file:/lib32/&#39;, name=&#39;lib32/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/libx32/&#39;, name=&#39;libx32/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/usr/&#39;, name=&#39;usr/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/home/&#39;, name=&#39;home/&#39;, size=4096, modificationTime=1652000065076),\n FileInfo(path=&#39;file:/sys/&#39;, name=&#39;sys/&#39;, size=0, modificationTime=1652000072368),\n FileInfo(path=&#39;file:/media/&#39;, name=&#39;media/&#39;, size=4096, modificationTime=1652000069620),\n FileInfo(path=&#39;file:/bin/&#39;, name=&#39;bin/&#39;, size=32768, modificationTime=1652000069648),\n FileInfo(path=&#39;file:/local_disk0/&#39;, name=&#39;local_disk0/&#39;, size=4096, modificationTime=1652000283238),\n FileInfo(path=&#39;file:/Workspace/&#39;, name=&#39;Workspace/&#39;, size=4096, modificationTime=1652000088635),\n FileInfo(path=&#39;file:/dbfs/&#39;, name=&#39;dbfs/&#39;, size=4096, modificationTime=1652000088453)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: [FileInfo(path=&#39;file:/dev/&#39;, name=&#39;dev/&#39;, size=520, modificationTime=1652000076348),\n FileInfo(path=&#39;file:/mnt/&#39;, name=&#39;mnt/&#39;, size=4096, modificationTime=1652000085733),\n FileInfo(path=&#39;file:/databricks/&#39;, name=&#39;databricks/&#39;, size=4096, modificationTime=1652000092325),\n FileInfo(path=&#39;file:/tmp/&#39;, name=&#39;tmp/&#39;, size=4096, modificationTime=1652005289956),\n FileInfo(path=&#39;file:/root/&#39;, name=&#39;root/&#39;, size=4096, modificationTime=1652000288566),\n FileInfo(path=&#39;file:/var/&#39;, name=&#39;var/&#39;, size=4096, modificationTime=1652000069892),\n FileInfo(path=&#39;file:/lib64/&#39;, name=&#39;lib64/&#39;, size=4096, modificationTime=1652000051751),\n FileInfo(path=&#39;file:/BUILD&#39;, name=&#39;BUILD&#39;, size=190, modificationTime=1652000042907),\n FileInfo(path=&#39;file:/run/&#39;, name=&#39;run/&#39;, size=500, modificationTime=1652000401355),\n FileInfo(path=&#39;file:/boot/&#39;, name=&#39;boot/&#39;, size=4096, modificationTime=1652000069616),\n FileInfo(path=&#39;file:/sbin/&#39;, name=&#39;sbin/&#39;, size=12288, modificationTime=1652000069808),\n FileInfo(path=&#39;file:/lib/&#39;, name=&#39;lib/&#39;, size=4096, modificationTime=1652000069652),\n FileInfo(path=&#39;file:/opt/&#39;, name=&#39;opt/&#39;, size=4096, modificationTime=1652000065192),\n FileInfo(path=&#39;file:/etc/&#39;, name=&#39;etc/&#39;, size=4096, modificationTime=1652000084405),\n FileInfo(path=&#39;file:/srv/&#39;, name=&#39;srv/&#39;, size=4096, modificationTime=1652000069620),\n FileInfo(path=&#39;file:/proc/&#39;, name=&#39;proc/&#39;, size=0, modificationTime=1652000072280),\n FileInfo(path=&#39;file:/lib32/&#39;, name=&#39;lib32/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/libx32/&#39;, name=&#39;libx32/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/usr/&#39;, name=&#39;usr/&#39;, size=4096, modificationTime=1652000069800),\n FileInfo(path=&#39;file:/home/&#39;, name=&#39;home/&#39;, size=4096, modificationTime=1652000065076),\n FileInfo(path=&#39;file:/sys/&#39;, name=&#39;sys/&#39;, size=0, modificationTime=1652000072368),\n FileInfo(path=&#39;file:/media/&#39;, name=&#39;media/&#39;, size=4096, modificationTime=1652000069620),\n FileInfo(path=&#39;file:/bin/&#39;, name=&#39;bin/&#39;, size=32768, modificationTime=1652000069648),\n FileInfo(path=&#39;file:/local_disk0/&#39;, name=&#39;local_disk0/&#39;, size=4096, modificationTime=1652000283238),\n FileInfo(path=&#39;file:/Workspace/&#39;, name=&#39;Workspace/&#39;, size=4096, modificationTime=1652000088635),\n FileInfo(path=&#39;file:/dbfs/&#39;, name=&#39;dbfs/&#39;, size=4096, modificationTime=1652000088453)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.put(\"file:/tmp/my_new_file\", \"This is a file on the local driver node.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eaa2a91-d195-4615-829b-37c703e1c3b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Wrote 40 bytes.\nOut[18]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 40 bytes.\nOut[18]: True</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Access files on mounted object storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4b2d2da-dbac-4e13-b601-466f9a8b58d6"}}},{"cell_type":"markdown","source":["- Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57c6af3d-7d25-4baa-a444-f061383479df"}}},{"cell_type":"code","source":["dbutils.fs.ls(\"/mnt/mymount\")\ndf = spark.read.text(\"dbfs:/mymount/my_file.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88c2730d-0cd3-4396-88dc-257ee16888e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3866784353822598&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>ls<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/mymount&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dbfs:/mymount/my_file.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.ls.\n: java.io.FileNotFoundException: File /8190330192940305/mnt/mymount does not exist.\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.listStatus(NativeAzureFileSystem.java:2492)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$ls$1(DBUtilsCore.scala:157)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:91)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:155)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"java.io.FileNotFoundException: File /8190330192940305/mnt/mymount does not exist.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3866784353822598&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>ls<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/mymount&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dbfs:/mymount/my_file.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.ls.\n: java.io.FileNotFoundException: File /8190330192940305/mnt/mymount does not exist.\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.listStatus(NativeAzureFileSystem.java:2492)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:510)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$ls$1(DBUtilsCore.scala:157)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:91)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:155)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## File upload interface"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3798aa4d-37db-47fd-abe7-eb7b19a5d8ee"}}},{"cell_type":"markdown","source":["- If you have small data files on your local machine that you want to analyze with Azure Databricks, you can easily import them to Databricks File System (DBFS) using one of the two file upload interfaces:\n    - from the DBFS file browser or \n    - from a notebook.\n\n- Files are uploaded to the FileStore directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b3ede52-02ac-4fa3-b291-63c8a5f22666"}}},{"cell_type":"markdown","source":["## Upload data to DBFS from the file browser\n- This feature is disabled by default. An administrator must enable the DBFS browser interface before you can use it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee27e24f-f507-4506-a64b-10909ba6575d"}}},{"cell_type":"markdown","source":["- Click Data Icon Data in the sidebar.\n- Click the DBFS button at the top of the page.\n- Click the Upload button at the top of the page.\n- On the Upload Data to DBFS dialog, optionally select a target directory or enter a new one.\n- In the Files box, drag and drop or use the file browser to select the local file to upload.\n![image.png](attachment:3a925635-2b3f-4c9e-8bf9-5e53dd3f3c21.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f179437b-021e-4532-bf22-e5527e131190"}}},{"cell_type":"markdown","source":["- Uploaded files are accessible by everyone who has access to the workspace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28fef104-ebea-4721-b714-07e0cf5232d9"}}},{"cell_type":"markdown","source":["## Upload data to DBFS from a notebook\n- This feature is enabled by default. If an administrator has disabled this feature, you will not have the option to upload files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fecb632b-36ff-4380-8b34-439bf5edbefb"}}},{"cell_type":"markdown","source":["- Create a new notebook or open an existing one, then click File > Upload Data\n![image.png](attachment:6095482e-ae48-4160-910d-cfc93436c862.png)![image.png](attachment:045c2850-f627-4ed0-83b9-94d215fd9870.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11e9a498-d433-4e3d-93fe-78c7ce5c2e01"}}},{"cell_type":"markdown","source":["- Select a target directory in DBFS to store the uploaded file. The target directory defaults to /shared_uploads/<your-email-address>/.\n    - Uploaded files are accessible by everyone who has access to the workspace.\n- Either drag files onto the drop target or click Browse to locate files in your local filesystem."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04f12ea0-c527-4761-b4dd-5ed2ade14e79"}}},{"cell_type":"markdown","source":["![image.png](attachment:6e052841-15a3-41ed-af9d-4944ba8c6998.png)![image.png](attachment:0baedeaa-2cb6-4228-b9c9-d9874f63a8d3.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2405a54-7923-457d-8cff-a2f828ab30a5"}}},{"cell_type":"markdown","source":["- When you have finished uploading the files, click Next.\n    - If youâ€™ve uploaded CSV, TSV, or JSON files, Azure Databricks generates code showing how to load the data into a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92246de9-2bcc-4e1d-a54a-4d9827513e7e"}}},{"cell_type":"markdown","source":["![image.png](attachment:c8a1fada-f4f5-41bb-8ec4-0fa204891971.png)\n\n- To save the text to your clipboard, click Copy."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84c3547c-908f-4f2e-8896-1ec86433ab11"}}},{"cell_type":"markdown","source":["- Click Done to return to the notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8029d40e-bd92-4c5d-9d1b-a995c03a4c62"}}},{"cell_type":"markdown","source":["## Databricks CLI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83da2e93-7c7d-402d-b6a3-2822b962ba9e"}}},{"cell_type":"code","source":["# List files in DBFS\ndbfs ls\n# Put local file ./apple.txt to dbfs:/apple.txt\ndbfs cp ./apple.txt dbfs:/apple.txt\n# Get dbfs:/apple.txt and save to local file ./apple.txt\ndbfs cp dbfs:/apple.txt ./apple.txt\n# Recursively put local dir ./banana to dbfs:/banana\ndbfs cp -r ./banana dbfs:/banana"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfd42a2d-aaf6-49a5-bef0-54c13f502299"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[1;36m  Input \u001B[1;32mIn [10]\u001B[1;36m\u001B[0m\n\u001B[1;33m    dbfs ls\u001B[0m\n\u001B[1;37m         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (3387470145.py, line 2)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[1;36m  Input \u001B[1;32mIn [10]\u001B[1;36m\u001B[0m\n\u001B[1;33m    dbfs ls\u001B[0m\n\u001B[1;37m         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## dbutils\n- dbutils.fs provides file-system-like commands to access files in DBFS.\n- To access the help menu for DBFS, use the dbutils.fs.help() command."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d03b4b6-0ba4-481b-81ea-1698b7bc68a0"}}},{"cell_type":"markdown","source":["### Write files to and read files from the DBFS root as if it were a local filesystem"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6722aef2-7655-497f-8370-6c46c0e45586"}}},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/foobar/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce14f694-5fab-475c-a367-99638b7598ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.put(\"/foobar/baz.txt\", \"Hello, World!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ac19a32-1ad5-4bf3-8809-f6aaa1616663"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.head(\"/foobar/baz.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5313cfe-c9f8-4a16-8d3f-c848ebec5caf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.rm(\"/foobar/baz.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"369987e2-189a-48a4-ad6c-d8976dd95bea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use dbfs:/ to access a DBFS path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a492d40-23e3-4b62-b8d7-3a66a0cfaefb"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/foobar\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da42a86e-6d39-49cd-990c-8b9b2edc7449"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use %fs magic commands"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c540a02a-aa16-4daa-b73b-4614d160728e"}}},{"cell_type":"code","source":["# List the DBFS root\n\n%fs ls\n\n# Recursively remove the files under foobar\n\n%fs rm -r foobar\n\n# Overwrite the file \"/mnt/my-file\" with the string \"Hello world!\"\n\n%fs put -f \"/mnt/my-file\" \"Hello world!\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d4bebdc-9cb5-4619-9b20-ea8c3f7f3949"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DBFS API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37ea931a-c750-464a-afa8-5c073227c5f7"}}},{"cell_type":"markdown","source":["- The following example writes the file foo.text to the DBFS /tmp directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f77f495-3c82-400e-a838-2e15ac05f9d4"}}},{"cell_type":"code","source":["df.write.text(\"/tmp/foo.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e475159f-5b0c-4de8-baad-bb6a95f6c3f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- To read a file named people.json in the DBFS location /FileStore, you can specify either of the following:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d25867a4-97c9-4f65-a37c-ac79b9e798d5"}}},{"cell_type":"code","source":["df = spark.read.json('dbfs:/FileStore/people.json')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32adb819-5eb1-4297-a5e3-3f71797293e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.json('/FileStore/people.json')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3556318a-2132-4c4d-9e6b-d7228e3d4459"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- The following does not work:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3922834-19b9-4ddf-a769-bc127d614337"}}},{"cell_type":"code","source":["# This will not work. The path must be absolute. It\n# must start with '/' or 'dbfs:/'.\ndf = spark.read.json('FileStore/people.json')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"534d98aa-3b2c-4f71-a507-3498b47c37dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[1;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)\nInput \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This will not work. The path must be absolute. It\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# must start with '/' or 'dbfs:/'.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFileStore/people.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[1;31mNameError\u001B[0m: name 'spark' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'spark' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[1;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)\nInput \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This will not work. The path must be absolute. It\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# must start with '/' or 'dbfs:/'.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFileStore/people.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[1;31mNameError\u001B[0m: name 'spark' is not defined"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Local file APIs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28db5f56-3b8c-46b2-8967-9f7f913469d9"}}},{"cell_type":"markdown","source":["- You can use local file APIs to read and write to DBFS paths.\n- Azure Databricks configures each cluster node with a FUSE mount /dbfs that allows processes running on cluster nodes to read and write to the underlying distributed storage layer with local file APIs.\n- When using local file APIs, you must provide the path under /dbfs. For example:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4271a7e1-524b-4a58-a5db-5b99cee57db8"}}},{"cell_type":"code","source":["#write a file to DBFS using Python file system APIs\nwith open(\"/dbfs/tmp/test_dbfs.txt\", 'w') as f:\n  f.write(\"Apache Spark is awesome!\\n\")\n  f.write(\"End of example!\")\n\n# read the file\nwith open(\"/dbfs/tmp/test_dbfs.txt\", \"r\") as f_read:\n  for line in f_read:\n    print(line)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07ec8fd0-1606-487c-bd67-d153ecae63bc"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.12","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"3-DBFS","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3866784353822577}},"nbformat":4,"nbformat_minor":0}
